/**********************************************************************************************************
		Prerequisites
***********************************************************************************************************/
1. Check latest JAVA is installed JAVA 7 or higher
2. check ssh is installed and up and running


/**********************************************************************************************************
		Install and configure hadoop
***********************************************************************************************************/
1.  wget http://mirrors.sonic.net/apache/hadoop/common/stable2/hadoop-2.7.4.tar.gz
2.  tar -xvzf hadoop-2.X.X.tar.gz
3.  mv hadoop-2.X.X/* hadoop/
4.  create hadoop directory
5.  create directory hdfs and inside it create 3 folders called namenode, datanode and tmp
6.  nano ~/.bashrc

# hadoop configuration
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/media/hduser/node/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

7.  source ~/.bashrc
8.  navidate to hadoop/etc/hadoop/ directory
9.  nano hadoop-env.sh 

export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
	
10. cp mapred.site.xml.template mapred.site.xml
11. nano hdfs-site.xml

<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default blockcareplication.</description>
 </property>
 <property>
  <name>dfs.namenode.name.dir</name>
  <value>file:/home/hduser/hdfs/namenode</value>
 </property>
 <property>
  <name>dfs.datanode.data.dir</name>
  <value>file:/home/hduser/hdfs/datanode</value>
 </property>
</configuration>

12. nano mapred-site.xml

<configuration>
 <property>
  <name>mapred.job.tracker</name>
  <value>localhost:54311</value>
  <description>The host and port that the MapReduce job tracker</description>
 </property>
</configuration>

13. nano core-site.xml

<configuration>
 <property>
  <name>hadoop.tmp.dir</name>
  <value>/home/hduser/hdfs/tmp</value>
  <description>A base for other temporary directories.</description>
 </property>
 <property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:54310</value>
  <description>The name of the default file system.</description>
 </property>
</configuration> 

/**********************************************************************************************************
		Testing hadoop
***********************************************************************************************************/

1.  hdfs or hadoop namenode -format
2.  start-dfs.sh
3.  start-yarn.sh
4.  run jps to check all demons are running
9026 NodeManager
7348 NameNode
9766 Jps
8887 ResourceManager
7507 DataNode

5.  Access the link http://localhost:50070/ to see the daemons
6.  stop-dfs.sh
7.  stop-yarn.sh

Runningg mapreduce in python
http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/
